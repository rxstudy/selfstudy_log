{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "\n",
    "k-armed bandit with $k=4$. Given a sequence of actions and rewards, if action $A_{i+1} = \\underset{a}{\\operatorname{argmax}} Q_i(a)$, then $A_{i+1}$ can be greedy or exploration. However if $A_{i+1} \\neq \\underset{a}{\\operatorname{argmax}} Q_i(a)$,  then $A_{i+1}$ can only be exploration.\n",
    "\n",
    "Given a list of $\\{(A_i, R_i)\\} = \\{ (1,-1), (2, 1), (2, -2), (2, 2), (3, 0) \\}$, we can get the $Q_i(a)$ using sample-average estimate.\n",
    "\n",
    "| Action      | Q1 | Q2 | Q3 | Q4 |\n",
    "| ----------- | ---| ---| ---| ---|\n",
    "|  -          | 0  |  0 |  0 | 0  |\n",
    "|  $A_1$    | -1  |  0 |  0 | 0  |\n",
    "|  $A_2$    | -1  |  1 |  0 | 0  |\n",
    "|  $A_3$    | -1  |-0.5|  0 | 0  |\n",
    "|  $A_4$    | -1  | 0.5|  0 | 0  |\n",
    "|  $A_5$    | -1  | 0.5|  0 | 0  |\n",
    " \n",
    "As we can see,  \n",
    "\n",
    "$A_1=1 \\in \\{1,2,3,4\\}= \\underset{a}{\\operatorname{argmax}} [0, 0, 0, 0]$ therefore might be greedy/exploration\n",
    "\n",
    "$A_2=2 \\in \\{2\\}= \\underset{a}{\\operatorname{argmax}} [-1, 1, 0, 0]$ therefore might be greedy/exploration\n",
    "\n",
    "$A_3=2 \\notin \\{3,4\\}= \\underset{a}{\\operatorname{argmax}} [-1, -0.5, 0, 0]$ therefore must be exploration\n",
    "\n",
    "$A_4=2 \\in \\{2\\}= \\underset{a}{\\operatorname{argmax}} [-1,  0.5, 0, 0]$ therefore might be greedy/exploration\n",
    "\n",
    "$A_5=3 \\notin \\{2\\} = \\underset{a}{\\operatorname{argmax}}[ -1, 0.5, 0, 0]$ therefore must be exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "In the long run, $\\epsilon = 0.01$ will perform the best. $E[R] = \\epsilon E[R_e] + (1 - \\epsilon) E[R_g]$ where $R$ is the total reward, $R_e$ is the total exploration reward and $R_g$ is the total greedy reward.  Note that For any $\\epsilon_1, \\epsilon_2 > 0$, $\\| E_{\\epsilon_1} [R_g] - E_{\\epsilon_2}[R_g] \\| \\rightarrow 0$ and greedy reward dominates exploration reward as $n$ goes to infinity, i.e $\\frac{\\|E[R_e]\\|}{\\|E[R_g]\\|} \\rightarrow 0$.\n",
    "\n",
    "Therefore,  $\\frac{E_{0.01}[R]}{E_{0.1}[R]} \\rightarrow \\frac{1 - 0.01}{1 - 0.1} = 110\\% $.  $\\epsilon = 0.01$ will perform about 10% better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
