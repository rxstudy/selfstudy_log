{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "1. Sudoku, State is 9x9 matrix with numbers 1-9. Actions, $A(s) = (i, j, a)$ where $i,j$ are availble positions and $k$ is the available number in that position in current state $s$. Reward $r$ is +1 for each grid being filled. In this example, actions are discrete and are dependent on the current state.\n",
    "2. Balance inverted pendulum, State is pole angle and cart velocity, action is force vector applied to the cart. Reward is a positive constant for each tick of time the pole managed to stay up. Here state is infinite as it is continuous.\n",
    "3. Chess, state is the board and actions are the currently allowed moves. Reward is 1 for winning, 0 for drawing and -1 for losing. The different is, the state depends on opponent's action as well and the reward is delayed until the game is ended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "\n",
    "MDP assume the state depends on previous state and action on that state $p(r', s'|s, a)$. If the state can be affected by random actions in the past in a delayed fashion. The dynamic function's dimension will blow up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "\n",
    "The level in which we model action space should be dependent on the goal of the learning tasks. \n",
    "The level should be the one that is closest to the level of where the goal is defined and one which we have full control.\n",
    "\n",
    "For example, if we want to minimize the driving distance from point A to point B. We have choice to define action as actions of the car, or the person inside the car or the person's brain activity. If we have full control of the car and since the car is on a macroscopic level closest to where the goal is defined, we will choose it. If we don't have full control of the car, say we are training a robot to drive it, the next cloest level which we have full control is the motor on the robot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "| $s$ | $a$     | $s'$   | $r$                  | $p(s', r |s, a)$ |\n",
    "|:- |:-     | :-  | :-                |:-|\n",
    "|high|search  | high  | $r_{search}$ |$\\alpha$    |\n",
    "|high|search  | low   | $r_{search}$ |1 - $\\alpha$|\n",
    "|high|wait    | high  | $r_{wait}$   |1           |\n",
    "|low |search  | low   | $r_{search}$ |$\\beta$     |\n",
    "|low |search  | high  | -3                  |1-$\\beta$   |\n",
    "|low |wait    | low   | $r_{wait}$   |1           |\n",
    "|low |recharge| high  | 0                   |1           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5\n",
    "\n",
    "$p(s', r | s, a) = Pr(S_t = s', R_t = r | S_{t-1}=s, A_{t-1}=a)$ 's domain needs to be changed to $s', s \\in S\\cup S^+$ and $p(s'| s', a) = 0$ when $s'\\in S^+$ for any $a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6\n",
    "\n",
    "For pole-balancing as episodic task, $G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{K-t-1}R_K$ where $K$ is the terminal time step. So $G_t = -\\gamma^{K-t-1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.7\n",
    "\n",
    "Treating running of maze as episodic tasks, the return $G_t = R_{t+1} + R_{t+2} + \\cdots + R_T$ where $T$ is the terminal time steps (We define $S^+$ to be only when agent escapes the maze). Based on the reward defined, $G_t = 1$ for any $t$ which means the return is constant for any episodes as they will have to reach the states in $S^+$. The agent will not know how to improve an already maximized return. \n",
    "\n",
    "With discount or penalty, the agent will have diminishing return the longer it takes to complete the maze. In this way, the agent will learn.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.8\n",
    "$G_5 = 0$\n",
    "\n",
    "$G_4 = R_5 + 0.5G_5 = 2$\n",
    "\n",
    "$G_3 = R_4 + 0.5G_4 = 3 + 1 = 4$\n",
    "\n",
    "$G_2 = R_3 + 0.5G_3 = 6 + 2 = 8$\n",
    "\n",
    "$G_1 = R_2 + 0.5G_2 = 2 + 4 = 6$\n",
    "\n",
    "$G_0 = R_1 + 0.5G_1 = -1 + 6 = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.9\n",
    "\n",
    "$G_0 = R_1 + \\sum_{k=2}^{\\infty}\\gamma^{k - 1}R_k = 2 + 7 \\sum_{k=1}^{\\infty}\\gamma^{k} = 2 + 7 \\times \\frac{0.9}{1-0.9} = 65$\n",
    "$G_0 = R_1 + \\gamma G_1 \\rightarrow G_1 = \\frac{G_0 - R_1}{\\gamma} = 70$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.10\n",
    "\n",
    "Take the limit of geometric series sum \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.11\n",
    "Let $R_{t+1}=r, S_{t+1}=s', S_t=s, A_t=a$, then\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "E[r|s] &= \\sum_r r p(r|s) \\\\\n",
    "       &= \\sum_r r \\sum_{s',a} p(s',r, s,a) \\\\ \n",
    "       &= \\sum_r r \\sum_{s',a} p(s',r | s,a) p(a|s)p(s) \\\\\n",
    "       &= \\sum_{r,s',a}r p(s',r | s,a) \\pi(a|s) \\ , \\  \\left(\\mbox{note that } p(s)=1 \\right) \\\\\n",
    "       &= \\sum_a \\pi(a|s) \\sum_{r,s'}r p(s',r | s,a) \n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.12\n",
    "\n",
    "$\\begin{align}\n",
    "v_{\\pi}(s) &= E_{\\pi}[G_t |S_t = s] \\\\ \n",
    "           &= E_{\\pi}[G_t p(G_t, A_t=a | S_t = s)] \\\\ \n",
    "           &= \\sum_{G_t, a} G_t p(G_t| A_t =a,  S_t = s)p(A_t=a|S_t=s) \\\\\n",
    "           &= \\sum_a p(A_t=a|S_t=s) \\sum_{G_t} G_t p(G_t| A_t =a,  S_t = s) \\\\\n",
    "           &= \\sum_a \\pi(a|s) q_{\\pi}(s, a)\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.13\n",
    "\n",
    "$\\begin{align}\n",
    "q_{\\pi}(s,a) &= E_{\\pi}[R_{t+1} + \\gamma G_{t+1} | s, a] \\\\\n",
    "        &= E_{\\pi}[R_{t+1}|s,a] + \\gamma E_{\\pi}[ G_{t+1} | s, a] \\\\\n",
    "        &= \\sum_r rp(r|s,a) + \\gamma \\sum_{s'}\\sum_{a'} E[G_{t+1} | s', a']p(s',a'|s,a) \\\\\n",
    "        &= \\sum_r rp(r|s,a) + \\gamma \\sum_{s'}\\sum_{a'} q_{\\pi}(s',a')p(s',a'|s,a) \\\\\n",
    "        &= \\sum_r \\sum_{s'} rp(r,s'|s,a) + \\gamma \\sum_{s'}\\sum_{a'} q_{\\pi}(s',a')p(a'|s',s,a)p(s'|s,a) \\\\\n",
    "        &= \\sum_r \\sum_{s'} rp(r,s'|s,a) + \\gamma \\sum_{s'}\\sum_{a'} q_{\\pi}(s',a')p(a'|s')\\sum_r p(r, s'|s,a) \\\\\n",
    "        &= \\sum_r \\sum_{s'} p(r,s'|s,a) \\left[r + \\gamma \\sum_{a'} q_{\\pi}(s',a')p(a'|s')\\right] \\\\\n",
    "        &= \\sum_r \\sum_{s'} p(r,s'|s,a) \\left[r + \\gamma \\sum_{a'} q_{\\pi}(s',a')\\pi(a'|s')\\right] \\\\\n",
    "        &= \\sum_r \\sum_{s'} p(r,s'|s,a) \\left[r + \\gamma v_{\\pi}(s') \\right] \\ \\ \\ \\ \\mbox{(By Exercise 3.12)} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.14\n",
    "For random policy, $\\pi(a|s) = 0.25$ for any $a$ and $s$. \n",
    "\n",
    "Let $s' \\in \\{l,r,u,d\\}$ denote the left,right,up,down from center and $a \\in \\{l, r, u, d\\}$ to denote the corresponding movement.\n",
    "\n",
    "$p(s', r=0 |s=center, a) = 1$ for $a = s'$ since the transition is deterministic and reward is always 0 for any direction from center. \n",
    "\n",
    "Therefore \n",
    "\n",
    "$\\begin{align} \n",
    "v_{\\pi}(center) &= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_{\\pi}(s')\\right] \\\\ \n",
    "   &= 0.25 \\sum_{s'}[0 + \\gamma v_{\\pi}(s')]  \\\\\n",
    "   &= \\sum_a 0.25 [0.9 \\times (0.7 + 2.3 + 0.4 - 0.4)] \\\\\n",
    "   &\\approx 0.7\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.15\n",
    "\n",
    "From (3.8), the return $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$. Adding a constant reward $c$, we have \n",
    "\n",
    "$\\hat{G}_t = \\sum_{k=0}^{\\infty} \\gamma^k (R_{t+k+1} + c) = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} +  c  \\sum_{k=0}^{\\infty} \\gamma^k = G_t + \\frac{c}{1-\\gamma}$\n",
    "\n",
    "Therefore \n",
    "\n",
    "$\\hat{v}(s) = E_{\\pi} [\\hat{G}_t |S_t = s] = E_{\\pi} [G_t + \\frac{c}{1-\\gamma} |S_t = s] = v(s)+ \\frac{c}{1-\\gamma}$\n",
    "\n",
    "$v_c =  \\frac{c}{1-\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.16\n",
    "\n",
    "Adding a constant $c$ to reward for episodic task, we don't have a converging infinite sum like we have in the conintuing tasks. \n",
    "So $\\hat{G}_t = \\sum_{k=0}^{T} \\gamma^k (R_{t+k+1} + c) = \\sum_{k=0}^{T} \\gamma^k R_{t+k+1} +  c  \\sum_{k=0}^{T} \\gamma^k = G_t + \\frac{c(1 - \\gamma^T)}{1-\\gamma}$\n",
    "\n",
    "And $\\hat{v}(s) = v(s) + c\\frac{1 - E[\\gamma^T| S_t = s]}{1-\\gamma}$ Depending on the random variable $T$\n",
    "\n",
    "For running a maze, if we have reward of -1 for each step the agent takes and +1 reward for escaping the maze. The agent can learn to minimize the step it takes to maximize the return. \n",
    "\n",
    "But if we add a 1 to all reward such that we have reward of 0 for each step the agent takes and reward of 2 for escaping the maze. The agent will not learn as the return will always be constant. (See exercise 3.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.17\n",
    "Bellman equation for action value\n",
    "\n",
    "$\\begin{align}\n",
    "q_{\\pi}(s,a) &= E_{\\pi}[G_{t} | s, a] \\\\\n",
    "  &= E_{\\pi}[R_{t+1} + \\gamma G_{t+1} | s, a] \\\\\n",
    "  &= E_{\\pi}[R_{t+1} | s, a] + \\gamma E_{\\pi}[G_{t+1} | s, a] \\\\\n",
    "  &= \\sum_r rp(r | s, a) + \\gamma \\sum_{a'}\\sum_{s'}E_{\\pi}[G_{t+1} | s', a']p(s',a'|s,a) \\\\\n",
    "  &= \\sum_r rp(r | s, a) + \\gamma \\sum_{a'}\\sum_{s'}q_{\\pi}(s',a')p(a'|s',s,a)p(s'|s, a) \\\\\n",
    "  &= \\sum_r \\sum_{s'} rp(s', r| s, a) + \\gamma \\sum_{a'}\\sum_{s'}q_{\\pi}(s',a')p(a'|s')\\sum_r p(s',r|s, a) \\\\\n",
    "  &= \\sum_r \\sum_{s'} rp(s', r| s, a) + \\gamma \\sum_{a'}\\sum_{s'}\\sum_r q_{\\pi}(s',a')\\pi(a'|s') p(s',r|s, a) \\\\\n",
    "  &= \\sum_r \\sum_{s'}p(s', r| s, a)\\left[ r + \\gamma \\sum_{a'} q_{\\pi}(s',a')\\pi(a'|s') \\right] \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.18\n",
    "\n",
    "From the diagram, $s$ branches out according to distribution $\\pi(a|s)$. Therefore,  $v_{\\pi}(a) = \\sum_a \\pi(a|s) q_{\\pi}(s,a) = E_{\\pi}[q_{\\pi}(s,a)|S_t=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.19\n",
    "\n",
    "$q_{\\pi}(s,a) = E[R_{t+1} + \\gamma v_{\\pi}(S_{t+1})|S_t = s, A_t = a]$\n",
    "\n",
    "$q_{\\pi}(s,a) = \\sum_{s',r} p(r, s'|s,a) \\left[r + \\gamma v_{\\pi}(s')\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.20\n",
    "\n",
    "The optimal value function is the same as $q_*(s, driver)$ in Figure 3.3 except it is -1 uniformly in the green area since it is -1 for using putt in the green area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.21\n",
    "It should be close to the example $v_{putt}(s)$ in Figure 3.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.22\n",
    "Since $q_*(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\max_{a'}q_*(s',a')]$\n",
    "\n",
    "Therefore $q_*(s, left) = 1 + \\gamma [0 + \\gamma \\max_{a'}q_*(s,a')] = 1 +  \\gamma^2 \\max\\{q_*(s, left), q_*(s, right)\\} $\n",
    "\n",
    "and $q_*(s, right) = 0 + \\gamma [2 + \\gamma \\max_{a'}q_*(s,a')] = 2 \\gamma +  \\gamma^2 \\max\\{q_*(s, left), q_*(s, right)\\}$\n",
    "\n",
    "$\\Delta = q_*(s, left) - q_*(s, right) = 1 - 2 \\gamma$\n",
    "\n",
    "When $\\gamma = 0$, $\\Delta = 1 - 0 > 1$,  so $\\pi_{left}$ is optimal\n",
    "\n",
    "When $\\gamma = 0.9$, $\\Delta = 1 - 2 \\times 0.9 < 0$ so $\\pi_{right}$ is optimal\n",
    "\n",
    "When $\\gamma = 0.5$,  $\\Delta = 1 - 2 \\times 0.5 = 0$ so both are optimal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.23\n",
    "\n",
    "Given $s\\in\\{h, l\\}$ and $a \\in \\{w, re, se\\}$,  $$q_*(s,a)=\\sum_{r,s'}p(s', r|s, a) \\left[r + \\gamma \\max_{a'}q_*(s',a') \\right]$$\n",
    "\n",
    "Let $q_{s,a} = q_*(s, a)$\n",
    "\n",
    "$q_{h,w} = r_w + \\gamma \\max\\{q_{h,w},q_{h,se}\\}$\n",
    "\n",
    "$q_{h,se} = \\alpha [r_{se} + \\gamma \\max\\{q_{h,w},q_{h,se}\\}] + (1-\\alpha) [r_{se} + \\gamma \\max \\{q_{l,w}, q_{l,se}, q_{l,re}\\}$\n",
    "\n",
    "$q_{l,se} = \\beta [r_{se} + \\gamma \\max\\{q_{l,w},q_{l,se}, q_{l,re}\\}] + (1-\\beta) [-3 + \\gamma \\max \\{q_{h,w}, q_{h,se}\\}$\n",
    "\n",
    "$q_{l,re} = \\gamma \\max\\{q_{h,w}, q_{h,se}\\}$\n",
    "\n",
    "$q_{l,w} = r_w + \\gamma \\max\\{q_{l,w},q_{l,se}, q_{l,re}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.24\n",
    "Denote the vertical path from $A$ to $A'$  as $(A, A_1,A_2,A_3, A')$. When at $A$, The optimal policy based on the computed state value (figure 3.5) is to loop between $A$ and $A'$ (After reaching $A'$,  moving vertically up back to $A$ is the shortest path).\n",
    "\n",
    "Let $v_x$ denote $v_*(x)$\n",
    "\n",
    "$v_A = \\max_a \\sum_{s', r} p(s', r|A, a) [ r + \\gamma v_{s'}] = 10 + \\gamma v_{A'}$\n",
    "\n",
    "$v_{A'} = 0 + \\gamma v_{A_3}$\n",
    "\n",
    "$v_{A_3} = \\gamma v_{A_2}$\n",
    "\n",
    "$v_{A_2} = \\gamma v_{A_1}$\n",
    "\n",
    "$v_{A_1} = \\gamma v_A$\n",
    "\n",
    "Therefore\n",
    "$v_A = 10 + \\gamma (\\gamma ( \\gamma ( \\gamma ( \\gamma v_A)))) = 10 + \\gamma^5 v_A \\Rightarrow v_A = \\frac{10}{1 - 0.9^5} = 24.419$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.25\n",
    "\n",
    "$\\begin{align}\n",
    " v_*(s) &= \\max_{\\pi}E_{\\pi}\\left[R_{t+1} + \\gamma G_{t+1} | S_t=s \\right] \\\\\n",
    "        &= \\max_{\\pi} \\sum_{a} E_{\\pi}\\left[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t = a \\right] p(a|s) \\\\\n",
    "        &= \\max_{\\pi} \\sum_{a} q_{\\pi}(s,a) \\pi(a|s)\n",
    "\\end{align}$\n",
    "\n",
    "Note that $\\sum_a \\pi(a|s) = 1$, so \n",
    "\n",
    "$\\begin{align}\n",
    "  \\sum_{a} q_{\\pi}(s,a) \\pi(a|s) &\\leq \\sum_{a} \\max_a \\{q_{\\pi}(s,a)\\} \\pi(a|s) \\\\ \n",
    "         &=  \\max_a \\{q_{\\pi}(s,a)\\} \\left(\\sum_{a} \\pi(a|s) \\right) \\\\ \n",
    "         &= \\max_a\\{q_{\\pi}(s,a)\\} \n",
    "  \\end{align} $\n",
    "\n",
    "The equality holds when $\\pi(a|s) = 1$ for the $a$ that maximizes $q_{\\pi}(s,a)$, hence\n",
    "$$ v_*(s) = \\sum_{a} q_{\\pi_*}(s,a) \\pi_*(a|s) = \\max_a q_*(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.26\n",
    "\n",
    "By Exercise 3.13,  $q_{\\pi}(s,a) = \\sum_{r,s'}p(r,s'|s,a)[r + \\gamma v_{\\pi}(s')]$\n",
    "\n",
    "$\\begin{align}\n",
    "q_*(s,a) &= \\max_{\\pi}q_{\\pi}(s,a) \\\\\n",
    "         &= \\max_{\\pi} \\sum_{r,s'}p(r,s'|s,a)[r + \\gamma v_{\\pi}(s')] \\\\\n",
    "         &=  \\sum_{r,s'}p(r,s'|s,a)[r + \\gamma \\max_{\\pi} v_{\\pi}(s')] \\\\\n",
    "         &=  \\sum_{r,s'}p(r,s'|s,a)[r + \\gamma v_{*}(s')] \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.27\n",
    "\n",
    "$\\begin{align}\n",
    "v_* &= \\max_{\\pi} v_{\\pi}(s) \\\\\n",
    "    &= \\max_{\\pi} E_{\\pi} [R_{t+1} + \\gamma G_{t+1} | s] \\\\\n",
    "    &= \\max_{\\pi} \\sum_a E_{\\pi} [R_{t+1} + \\gamma G_{t+1} | a, s] p(a|s) \\\\\n",
    "    &= \\max_{\\pi} \\sum_a q_{\\pi}(s,a) \\pi(a|s) \\\\\n",
    "    &\\leq \\max_{\\pi} \\max_a q_{\\pi}(s,a) \\\\\n",
    "    &= \\max_a q_{*}(s,a)  \\\\\n",
    "\\end{align}$\n",
    "\n",
    "As in 3.25,  $\\sum_a q_{\\pi}(s,a) \\pi(a|s) \\leq \\max_a \\{q_{\\pi}(s,a)\\}\\sum_a \\pi(a|s) = \\max_a \\{q_{\\pi}(s,a)\\} $\n",
    "\n",
    "The equality holds only when $ \\pi(a_*|s)= 1$ for the action that maximizes $q_{\\pi}(s,a)$ and 0 everywhere else. \n",
    "\n",
    "This policy is the optimal policy. Therefore \n",
    "$$\\pi_*(a|s) =  \\begin{cases}\n",
    "    1,& \\text{if } a = \\arg \\max_a q_*(s,a) \\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}  $$\n",
    "\n",
    "#### Notice that after getting the optimal action-value function, we can use greedy algorithm on the current $q_*(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.28\n",
    "\n",
    "$\\begin{align}\n",
    "v_* &= \\max_{\\pi} v_{\\pi}(s) \\\\\n",
    "    &= \\max_{\\pi} E_{\\pi}[R_{t+1} + \\gamma (R_{t+2} + \\gamma G_{t+2})|s] \\\\\n",
    "    &= \\max_{\\pi} \\left[ E_{\\pi}[R_{t+1}|s] + \\gamma \\sum_{s'} E_{\\pi}[ R_{t+2} + \\gamma G_{t+2} |s'] p(s'|s) \\right] \\\\\n",
    "    &= \\max_{\\pi} \\left[ E_{\\pi}[R_{t+1}|s] + \\gamma \\sum_{s',r,a} v_{\\pi}(s')p(s',r,a|s) \\right] \\\\\n",
    "    &= \\max_{\\pi} \\left[ E_{\\pi}[R_{t+1}|s] + \\gamma \\sum_{s',r,a} v_{\\pi}(s')p(s',r|s,a)p(a|s) \\right] \\\\\n",
    "    &= \\max_{\\pi} \\left[ \\sum_{s',r,a} r p(s',r|s,a)p(a|s) + \\gamma \\sum_{s',r,a} v_{\\pi}(s')p(s',r|s,a)p(a|s) \\right] \\\\\n",
    "    &= \\max_{\\pi} \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_{\\pi}(s') \\right] \\\\\n",
    "    &\\leq \\max_{\\pi} \\max_a \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_{\\pi}(s') \\right] \\\\\n",
    "    &= \\max_a \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_{*}(s') \\right] \n",
    "\\end{align}$\n",
    "\n",
    "Equality holds when $\\pi(a_*|s) = 1$ for $a_* = \\arg \\max _a  \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_{*}(s') \\right] $ and 0 otherwise.\n",
    "\n",
    "#### Notice that after getting the optimal value function, we need to search one step forward in order to come up with the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.29\n",
    "With $p(s'|s,a)$ and $r(s,a) = E[R_{t+1}|s]$\n",
    "\n",
    "$\\begin{align}\n",
    "v_{\\pi}(s) &= E_{\\pi}[R_{t+1} + \\gamma G_{t+1} | s] \\\\\n",
    "        &= E_{\\pi}[R_{t+1}|s] + \\gamma  E_{\\pi}[R_{t+2} + \\gamma G_{t+2} | s] \\\\\n",
    "        &= \\sum_a E_{\\pi}[R_{t+1}|s,a]\\pi(a|s) + \\gamma \\sum_{s'} E_{\\pi}[R_{t+2} + \\gamma G_{t+2} | s']p(s'|s) \\\\\n",
    "        &= \\sum_a r(s,a) \\pi(a|s) + \\gamma \\sum_{s',a} v_{\\pi}(s') p(s'|s,a)\\pi(a|s) \\\\\n",
    "        &= \\sum_a \\pi(a|s) \\left[r(s,a)  + \\gamma \\sum_{s'} v_{\\pi}(s') p(s'|s,a) \\right]  \\\\\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "v_{*}(s) &= \\max_{\\pi} v_{\\pi}(s) \\\\\n",
    "        &= \\max_{\\pi} \\sum_a \\pi(a|s) \\left[r(s,a)  + \\gamma \\sum_{s'} v_{\\pi}(s') p(s'|s,a) \\right] \\\\\n",
    "        &= \\max_a \\left[r(s,a)  + \\gamma \\sum_{s'} v_*(s') p(s'|s,a) \\right] \\\\\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "q_{\\pi}(s,a) &= E_{\\pi}[R_{t+1} + \\gamma G_{t+1} | s,a] \\\\\n",
    "        &= E_{\\pi}[R_{t+1}|s,a] + \\gamma  E_{\\pi}[R_{t+2} + \\gamma G_{t+2} | s,a] \\\\\n",
    "        &= r(s,a) + \\gamma \\sum_{s',a'} E_{\\pi}[R_{t+2} + \\gamma G_{t+2} | s',a']p(s',a'|s,a) \\\\\n",
    "        &= r(s,a) +\\gamma \\sum_{s',a'} q_{\\pi}(s',a') p(s',a'|s,a) \\\\\n",
    "        &= r(s,a) +\\gamma \\sum_{s',a'} q_{\\pi}(s',a') p(a'|s', s,a)p(s'|s,a) \\\\\n",
    "        &= r(s,a) +\\gamma \\sum_{s',a'} q_{\\pi}(s',a') \\pi(a'|s')p(s'|s,a) \\\\\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "q_{*}(s,a) &= \\max_{\\pi} q_{\\pi}(s,a) \\\\\n",
    "        &= \\max_{\\pi} \\left[ r(s,a) +\\gamma \\sum_{s',a'} q_{\\pi}(s',a') \\pi(a'|s')p(s'|s,a) \\right] \\\\\n",
    "        &= r(s,a) +\\gamma \\max_{a'} \\sum_{s'} q_{\\pi}(s',a') p(s'|s,a) \\\\\n",
    "\\end{align}$\n",
    "\n",
    "The best policy is to set the optimal action to have probability of 1 and 0 otherwise as shown in the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
