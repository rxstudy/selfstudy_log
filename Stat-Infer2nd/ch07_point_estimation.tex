\documentclass[12pt]{article}

\input{../definitions.tex}

\title{Chapter 7: Point Estimation}
\begin{document}
	\maketitle
	
\section*{Exercise 7.1}
 If we take the product of $f(x|\theta)$ as the likelihood function, all 3 values of $\theta$ attain maximum likelihood of 0. 
 
 \subsubsection*{Method 1: With perturbation on likelihood function}
 We will perturb the likelihood function by $\epsilon$. Now even the probability 0 entries get a probability of $\epsilon$. Then 
 \begin{equation*}
 	\begin{split}
 		L(\theta=1 | x + \epsilon) &= \frac{\epsilon (1/3 + \epsilon )^2 (1/6 + \epsilon)^2}{(1 + 5\epsilon)^5} \\
 		L(\theta=2 | x + \epsilon) &= \frac{\epsilon (1/4 + \epsilon )^4}{(1 + 5\epsilon)^5} \\
 		L(\theta=3 | x + \epsilon) &= \frac{\epsilon^2 (1/4 + \epsilon )^2 (1/2 + \epsilon)}{(1 + 5\epsilon)^5} \\
 	\end{split}
 \end{equation*}
 We can compare the functions by taking the ratio and letting $\epsilon$ go to 0.  
 $$
 \frac{ L(\theta = 2 | x ) }{ L(\theta = 1 | x) } = \lim_{\epsilon \rightarrow 0} \frac{ L(\theta = 2 | x + \epsilon) }{ L(\theta = 1 | x + \epsilon) } = \lim_{\epsilon \rightarrow 0} \frac{(1/4 + \epsilon )^4}{(1/3 + \epsilon )^2 (1/6 + \epsilon)^2} = 1.26
 $$

 Similarly, $$
 \frac{L(\theta = 2| x)} {L(\theta = 3 | x)} = \lim_{\epsilon \rightarrow 0} \frac{(1/4 + \epsilon )^4}{\epsilon (1/4 + \epsilon )^2 (1/2 + \epsilon)} = \infty$$
 
Therefore $\theta=2$ is the MLE

 \subsubsection*{Method 2: With +1 smoothing}
 we can assume $N$ trials are performed for each $\theta$ and apply laplace smoothing (+1 smoothing). 
 
 Then the likelihood functions become 
 \begin{equation*}
 	\begin{split}
     L(\theta=1|x) &= \lim_{N \rightarrow \infty}\frac{(N/3 + 1)^2(N/6 + 1)^2}{(N + 5)^5} \\
 	L(\theta=2|x) &= \lim_{N \rightarrow \infty}\frac{(N/4 + 1)^4}{(N + 5)^5} \\
 	L(\theta=3|x) &= \lim_{N \rightarrow \infty}\frac{(N/4 + 1/ N)^2(N/2 + 1)}{(N + 5)^5} \\
 	\end{split}
 \end{equation*}
This is essentially the same as method 1.


\section*{Exercise 7.2}
 Gamma$(\alpha, \beta)$ has pdf $f(x|\alpha, \beta) = \frac{x^{\alpha - 1} e^{-\beta x} \beta^\alpha}{\Gamma(\alpha)}$, with $\alpha$ known, the likelihood function is $$
   L(\alpha, \beta|x) = \prod^n_{i=1} f(x_i |\alpha, \beta) 
   = \prod^n_{i=1}\frac{x_i^{\alpha - 1} e^{-\beta x_i}\beta^\alpha}{\Gamma(\alpha)} = \frac{(\prod_i x_i)^{\alpha - 1}}{\Gamma^n(\alpha)} \left[ \beta^{n \alpha} e^{- \beta (\sum_i x_i)}\right]
 $$
 Let $\frac{\partial L(\beta|x, \alpha)}{\partial \beta} = 0$, we get the MLE for $\beta$. $$\hat{\beta} = \frac{\alpha}{\bar{x}}$$
 
If $\alpha$ is also unknown,

$$
  \frac{\partial L(\alpha, \beta |x)}{\partial \alpha} =  
  \frac{\partial}{\partial \alpha}  \frac{A^{\alpha - 1}  \beta^{n \alpha}}{\Gamma^n(\alpha)} = 0 $$ 
$$
  \Rightarrow  \sum \ln x_i + n \ln \beta = \psi (\alpha)
   \Rightarrow \sum \ln x_i + n (\ln \alpha - ln \bar{x}) = \psi (\alpha)
$$
 where $\psi$ is polygamma function

We can then solve for $\alpha$ numerically.

\section*{Exercise 7.3}
$\log$ is monotonic increasing function, when $L$ attains maximum values, $\log (L)$ also attains maximum.


\section*{Exercise 7.6}
$f(x|\theta) = \theta x^{-2}$ with $0 < \theta \leq x < \infty$. We need to extend the range of $x$ in order to get rid of the $\theta$ dependency with indicator function. So $f(x | \theta) =  \mathbb{I}_{\theta \leq x} \theta x^{-2}$ and $x \in \mathbb{R}$. 

(a) For a sample $\varvec{X}$, by factorization theorem, $f(x|\theta) = g(T(x)|\theta)h(x)$. So $h(x) = x^-2$ and $g(T(x)|\theta) =  \mathbb{I}_{\theta \leq x} \theta = \mathbb{I}_{\theta \leq x_i,  \forall i} \theta = \mathbb{I}_{\theta \leq \min{x_i} }$. Therefore $T(x) = \min{x_i}$ is a sufficient statistics.

(b) The likelihood function $L(\theta | x) = \mathbb{I}_{\theta \leq x_i} \theta^n (\prod_i x_i)^{-2}$. $L(\theta | x)$ is non zero only when $\theta \leq \min{x_i}$. Then $L(\theta | x) = \theta^n (\prod_i x_i)^{-2} $ when $\theta \leq \min{x_i}$ decreases monotonically as $\theta$ decreases. So it attains maximum when $\theta = \min{x_i}$


\end{document}
